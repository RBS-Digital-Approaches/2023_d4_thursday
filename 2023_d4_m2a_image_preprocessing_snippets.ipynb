{"cells":[{"cell_type":"markdown","metadata":{"id":"2beKMvl0-co-"},"source":["# Image pre-processing\n","When we think about the challenges of optical character recognition (OCR) of rare materials, we often think about the difficulty of training a language model to recognize archaic letterforms or to handle the inherent variability of hand-set type.\n","\n","Those are serious questions, for sure, but it turns out that one of the biggest determinants of OCR quality is the quality of the images that we attempt to run OCR on.\n","\n","This all means that the pre-processing of images *before* they're OCR'ed can be at least as important as having a good OCR training.\n","\n","This notebook and the following one walk through pre-processing steps to prepare images in hopes of getting the best result we can from our OCR training.\n","\n","In this notebook, we'll experiment with thresholding, and then work to crop the images to exclude portions of the page that don't include text.\n","\n","In the next notebook, we'll work to straighten any skewing in the images so that the lines of text are more readily recognized by the OCR software. In that notebook we'll also convert the images to black and white so that we can experiment with performing OCR on them.\n","\n",">*Note:* Your attention **really** doesn't need to be on the details of the code, itself, in these notebooks. *Most* of the code in this notebook ends up being devoted not to performing the transformations we're actually after, but simply to showing the stages of the transformation in the browser.\n",">\n","> In the first part of the notebook, focus on the different variables that affect the way that the images are transformed. I've set the notebook up so that you can change variables easily and re-process images to see what difference your changes make.\n",">\n",">In the second, longer, part of the notebook, focus on the strategies and heuristics involved in identifying the text block, even if the details of the code seem obscure.\n"]},{"cell_type":"markdown","metadata":{"id":"Vu_bP0HTaoWQ"},"source":["###A - Move images from Google Drive to Colaboratory environment\n","**Note**: Make sure you've added the RBS shared folder to your Google Drive."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RW7I14_-iGdG"},"outputs":[],"source":["#Code cell #1\n","#Connect to Google Drive\n","from google.colab import drive\n","drive.mount('/gdrive')\n","\n","#Import libraries to allow interactive widgets in this notebook\n","import ipywidgets as widgets\n","from ipywidgets import interact, interact_manual, interactive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O6UgA40sa785"},"outputs":[],"source":["#Code cell #2\n","%cp /gdrive/MyDrive/L-100\\ Digital\\ Approaches\\ to\\ Bibliography\\ \\&\\ Book\\ History-2023/2023_page_images.zip /content/2023_page_images.zip\n","%cd /content/\n","!unzip 2023_page_images.zip\n","%cd /content/2023_page_images/\n","%ls -al"]},{"cell_type":"markdown","metadata":{"id":"VUxKwY1ODmgf"},"source":["### B - Setting our source image"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"2hhoGYSwr2xD"},"outputs":[],"source":["#@title Select an image\n"," #@markdown **Run this cell** to create a select list widget that allows us to choose an image to process. With an image selected (a default is provided), you can continue working through the code below.\n","\n"," #@markdown You only need to run this cell once (re-running it will just set things back to the default value). But you can change the image you're working with using the select list in order to see how these processes work given different starting images.\n","import os\n","import glob\n","file_list = sorted([os.path.basename(file) for file in glob.glob('/content/2023_page_images/*')])\n","image_select = widgets.Dropdown(\n","    description='Choose image',\\\n","    options = file_list,\\\n","    value = '1730f_p13.tif',\n","    style={'description_width': 'initial'})\n","display(image_select)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cuK11EH4iifj"},"outputs":[],"source":["#Code cell #3\n","source_directory = '/content/2023_page_images/'\n","source_image = source_directory + image_select.value\n"]},{"cell_type":"markdown","metadata":{"id":"6uZZXFKPELMC"},"source":["## 1 - Getting the idea of binarizing: Setting a threshold manually"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gqenI-mDiJLZ"},"outputs":[],"source":["#Code cell #4\n","# !pip install pillow\n","#Install the Image and ImageDraw libraries from PIL (actually Pillow)\n","from PIL import Image, ImageDraw\n","\n","#Open the original color image\n","pilcolor_image = Image.open(source_image)\n","pilcolor_image"]},{"cell_type":"markdown","metadata":{"id":"ejBJVoGeQKCV"},"source":["### 1.a - Converting from color to grayscale"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pFQB36XYiqE1"},"outputs":[],"source":["#Code cell #5\n","#Use the Image library's convert() method to convert the color image to grayscale\n","pilgray_image = pilcolor_image.convert('L')\n","\n","#Output\n","pilgray_image"]},{"cell_type":"markdown","metadata":{"id":"avdYngYcG3Lv"},"source":["### 1.b - Converting our grayscale image to black and white\n","This isn't going to work the way we might expect..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mJP5AxQak30S"},"outputs":[],"source":["#Code cell #6\n","pilbw_image = pilgray_image.convert('1')\n","pilbw_image"]},{"cell_type":"markdown","metadata":{"id":"llwzJK5yIUDZ"},"source":["#### 1.b.i - Overriding the default behavior: turning off dithering\n","See [the documentation](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.convert):\n",">The default method of converting a greyscale (“L”) or “RGB” image into a bilevel (mode “1”) image uses Floyd-Steinberg dither to approximate the original image luminosity levels. If dither is NONE, all values larger than 127 are set to 255 (white), all other values to 0 (black). To use other thresholds, use the point() method."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"urV_mOpcOEgs"},"outputs":[],"source":["#Code cell #7\n","pilbw_image = pilgray_image.convert('1', dither=0)\n","pilbw_image"]},{"cell_type":"markdown","metadata":{"id":"rmRGuTcjPLKH"},"source":["#### 1.b.ii - Adjusting the threshold point manually\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DxODAI8C9tXo"},"outputs":[],"source":[" #@title Set a threshold value {display-mode: \"form\"}\n"," #@markdown Run this cell, then enter a value between 0 and 255 and hit the \"enter\" key to adjust the threshold point for our image in the cell below. (We begin with a default of 150)\n","\n"," #@markdown You only need to run this cell once (re-running it will just set things back to the default value). Try entering a new value and then re-running the *next* cell a few times to see the difference that different threshold values make.\n"," thresh_value_int = widgets.BoundedIntText(\n","    value=150,\n","    min=0,\n","    max=255,\n","    step=1,\n","    description='Threshold:',\n","    disabled=False\n",")\n"," display(thresh_value_int)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NmfLczNdm2yD"},"outputs":[],"source":["#Code cell #8\n","#See https://stackoverflow.com/questions/9506841/using-python-pil-to-turn-a-rgb-image-into-a-pure-black-and-white-image/50090612#50090612\n","\n","#Get the current value of our slider widget from the cell above\n","thresh = thresh_value_int.value\n","\n","#Look at every pixel of our image. If the value of that pixel is\n","#greater than the \"thresh\" value (set by the slider above), set the pixel to\n","#255 (pure black). Otherwise, set the pixel to 0 (pure white)\n","fn = lambda x : 255 if x > thresh else 0\n","\n","#Convert our image, overriding the default dithering behavior\n","#with the threshold we've chosen, using the lambda function from line 10\n","pilbinary_image = pilgray_image.convert('L').point(fn, mode='1')\n","\n","pilbinary_image"]},{"cell_type":"markdown","metadata":{"id":"RnAXVGtvQzHL"},"source":["## Interlude - Automating image optimization"]},{"cell_type":"markdown","metadata":{"id":"LMaG3ApkKaiR"},"source":["###We are immediately going to run into a problem..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_DmvvP1xoXIN"},"outputs":[],"source":["#Code cell #9\n","# !pip install opencv-python\n","import cv2\n","#This is a Google Colab-specific patch to enable us to view OpenCV images\n","#in the browser\n","from google.colab.patches import cv2_imshow\n","#Other Python libraries that CV2 needs\n","import matplotlib.pyplot as plt\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BDZ1pmODLpc_"},"outputs":[],"source":["#Code cell 10\n","#Open the original color image\n","cv2color_image = cv2.imread(source_image, cv2.IMREAD_COLOR)\n","#Convert to grayscale\n","cv2gray_image = cv2.cvtColor(cv2color_image, cv2.COLOR_BGR2GRAY)\n","#Apply Gaussian blur\n","cv2blurred_image = cv2.GaussianBlur(cv2gray_image, (5, 5), 0)\n","#Threshold using Otsu's method\n","(T_orig, cv2binary_otsu_image) = cv2.threshold(cv2blurred_image, 0, 255, cv2.THRESH_OTSU)\n","#Show binarized image\n","cv2_imshow(cv2binary_otsu_image)\n","print('Otsu threshold value: ' + str(T_orig))"]},{"cell_type":"markdown","metadata":{"id":"gxFDyfHRP9tB"},"source":["##2 - Cropping pages\n","Buckle up, because here's where things start to get weird."]},{"cell_type":"markdown","metadata":{"id":"cbeSYbhxUbm5"},"source":["####2.a - Invert the image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HC3hNbviwHp9"},"outputs":[],"source":["#Code cell 11\n","#This sets anything above a threshold level to true black, and combines two types\n","#of thresholding (THRESH_BINARY_ENV and the OpenCV implementation of Otsu's method).\n","invert_image = cv2.threshold(cv2blurred_image, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n","cv2_imshow(invert_image)"]},{"cell_type":"code","source":["#Code cell #12\n","#Let's get rid of the (now) white border around the page image\n","#See: https://learnopencv.com/filling-holes-in-an-image-using-opencv-python-c/\n","h, w = invert_image.shape[:2]\n","mask = np.zeros((h+2, w+2), np.uint8)\n","floodfill_image = invert_image.copy()\n"," # Floodfill from point (0, 0)\n","cv2.floodFill(floodfill_image, mask, (0,0), 0);\n","cv2_imshow(floodfill_image)"],"metadata":{"id":"IyRpGlsAKRV7"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PYfHf8DVwvMB"},"outputs":[],"source":["#@title Set kernel size for dilation {display-mode: \"form\"}\n","\n","#@markdown **Run the code** in this cell to create a set of\n","#@markdown slider widgets for changing the values of the\n","#@markdown \"kernel\" used to dilate the white pixels in the\n","#@markdown image. You can change the height and width of the\n","#@markdown kernel (i.e., the amount of vertical and horizontal\n","#@markdown dilation to be applied) as well as the number of\n","#@markdown iterations (how many times the dilation operation\n","#@markdown will be applied.)\n","\n","#@markdown You only need to run this cell once (re-running\n","#@markdown it will just re-set the values to their defaults).\n","#@markdown You can change the values of the sliders and\n","#@markdown then run Code cell 12 to see the different\n","#@markdown effects that different values have.\n","kernel_width = widgets.IntSlider(description = 'Kernel width', \\\n","                                               min=1, max=25, step=1, value=10)\n","kernel_height = widgets.IntSlider(description='Kernel height', \\\n","                                                 min=1, max=25, step=1, value=20)\n","num_iterations = widgets.IntSlider(description='Iterations', min=1, \\\n","                      max=10, step=1, value=5)\n","display(kernel_width)\n","display(kernel_height)\n","display(num_iterations)"]},{"cell_type":"markdown","metadata":{"id":"00UF6dq1aqGq"},"source":["####2.b - Dilate the inverted image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i5OAHmcCvzW9"},"outputs":[],"source":["#Code cell 13\n","#The shape for dilating pixels\n","kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_width.value, kernel_height.value))\n","#Create a new image by dilating the prior image using the kernel shape we've set\n","dilate_image = floodfill_image.copy()\n","dilate_image = cv2.dilate(dilate_image, kernel, iterations=num_iterations.value)\n","cv2_imshow(dilate_image)"]},{"cell_type":"markdown","metadata":{"id":"NWupAW9bbbM2"},"source":["###2.c - Identify contours of dilated regions\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kDPFo4hBOQez"},"outputs":[],"source":["#Code cell 14\n","#Identify the contours and their hierarchy\n","contours, hierarchy = cv2.findContours(dilate_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","\n","#These lines are just to visualize what we have. We make a copy of the dilate\n","#image, converting it from binary to color (so we can see colored lines on it),\n","#then draw all of the contours on that new image in green.\n","show_contours = cv2.cvtColor(dilate_image.copy(), cv2.COLOR_BayerGR2RGB)\n","show_contours = cv2.drawContours(show_contours, contours, -1, (0,255,0), 3)\n","cv2_imshow(show_contours)"]},{"cell_type":"markdown","metadata":{"id":"MpVQrE3ScwBD"},"source":["###2.d - Defining contours of interest"]},{"cell_type":"markdown","metadata":{"id":"wIh8CZqUkMdm"},"source":["####2.d.i - Define a region in the center of the page"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rQj3ZWEjKO0I"},"outputs":[],"source":["#Code cell 15\n","#Get the height and width of the image\n","height = np.shape(dilate_image)[0]\n","width = np.shape(dilate_image)[1]\n","\n","#Divide the width by 8\n","eighth = int(width/8)\n","#Find the midpoint on the x-axis\n","midpoint_x = int(width/2)\n","#Create a tuple with the left-most and right-most x-axis for this zone\n","middle_zone = (midpoint_x - eighth, midpoint_x + eighth)\n","\n","#This code is just to display what's going on. We make a copy of the image\n","#that already has our contours drawn in green...\n","show_middle_zone = show_contours.copy()\n","#...then draw two blue lines to show the edges of the middle zone\n","show_middle_zone = cv2.line(show_middle_zone, (middle_zone[0],0),\n","                            (middle_zone[0], height), (255,0,0), 3)\n","show_middle_zone = cv2.line(show_middle_zone, (middle_zone[1],0),\n","                            (middle_zone[1],height), (255,0,0), 3)\n","cv2_imshow(show_middle_zone)"]},{"cell_type":"markdown","metadata":{"id":"ECZ-V9b6fvae"},"source":["####2.d.ii - Identify contours centered on the area of interest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Nrf3nZ1LpRK"},"outputs":[],"source":["#Code cell 16\n","#Create an empty list\n","middle_zone_contours = []\n","#Iterate through the list of contours\n","for contour in contours :\n","  #https://learnopencv.com/find-center-of-blob-centroid-using-opencv-cpp-python/\n","  M = cv2.moments(contour)\n","  contour_x = int(M[\"m10\"] / M[\"m00\"])\n","  #If the x-axis value of the centroid is in range for teh x-axis values of the\n","  #middle_zone, then add it to the list of middle_zone_contours\n","  if middle_zone[0] <= contour_x <= middle_zone[1] :\n","    middle_zone_contours.append(contour)\n","\n","#This code just shows what we've done\n","show_middle_contours = show_middle_zone.copy()\n","\n","#Iterate through the list of middle_zone_contours, outlining them in purple\n","for middle_contour in middle_zone_contours :\n","  show_middle_contours = cv2.drawContours(show_middle_contours, [middle_contour], -1, (255, 0, 255), 3)\n","cv2_imshow(show_middle_contours)"]},{"cell_type":"markdown","metadata":{"id":"gOGCZISsk5QC"},"source":["####2.d.iii - Find boundary rectangles for contours of interest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Aib099heOyhg"},"outputs":[],"source":["#Code cell 17\n","\n","#The image is getting a little busy, so I'm making a new copy of the dilate image,\n","#converting it to color to be able to show colored lines and rectangles\n","show_rectangles = cv2.cvtColor(dilate_image.copy(), cv2.COLOR_BayerGR2BGR)\n","#Put the detected contours back on the image\n","for contour in contours :\n","  #All contours in green\n","  show_rectangles = cv2.drawContours(show_rectangles, contour, -1, (0,255,0), 3)\n","for middle_zone_contour in middle_zone_contours :\n","  #Middle zone contours in purple\n","  show_rectangles = cv2.drawContours(show_rectangles, [middle_zone_contour], -1, (255, 0, 255), 3)\n","\n","#Create a list of rectangles founding by getting the boundingRect of each contour\n","#in the middle_zone\n","rectangles = [cv2.boundingRect(contour) for contour in middle_zone_contours]\n","for rectangle in rectangles :\n","\n","  #openCV stores boundingRects as a tuple consisting of the x, y coordinate of\n","  #the upper left corner, the width of the rectangle, and the height of the rectangle.\n","  #But it *draws* rectangles based on two points: the upper left corner and\n","  #the lower right corner. So these lines figure out those two points for each\n","  #rectangle\n","  start_point = (rectangle[0], rectangle[1])\n","  end_point = (rectangle[0] + rectangle[2], rectangle[1] + rectangle[3])\n","  #Then draw the rectangle on the image\n","  show_rectangles = cv2.rectangle(show_rectangles, start_point, end_point, (0, 0, 255), 3)\n","  #And print the coordinates of the upper left corner\n","  show_rectangles = cv2.putText(show_rectangles, str(rectangle[0]) + ',' + str(rectangle[1]),\n","                              (rectangle[0], rectangle[1]),\n","                              cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 2)\n","\n","cv2_imshow(show_rectangles)"]},{"cell_type":"markdown","metadata":{"id":"w3DMPfhAoS6P"},"source":["####2.d.iv - Determine a rectangle large enough to contain all of the boundary rectangles of the contours of interest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pXGsmzwv5Est"},"outputs":[],"source":["#Code cell 18\n","#Construct lists of x- and y-axis coordinates for each rectangle\n","leftx_coords = [rectangle[0] for rectangle in rectangles]\n","rightx_coords = [rectangle[0] + rectangle[2] for rectangle in rectangles]\n","topy_coords = [rectangle[1] for rectangle in rectangles]\n","bottomy_coords = [rectangle[1]  + rectangle[3] for rectangle in rectangles]\n","\n","#Get the left-, right-, top-, and bottom-most x- and y-axis values by getting\n","#the minima and maxima of the values in the lists we just made, then\n","#padding them a little bit so that we're not cropping right against the text\n","leftmost = min(leftx_coords) - 100\n","rightmost = max(rightx_coords) + 100\n","topmost = min(topy_coords) - 50\n","bottommost = max(bottomy_coords) + 50\n","\n","#Construct coordinates for the four corners of the imaginary rectangle using the\n","#left-, right-, top-, and bottom-most x- and y-axis values\n","upper_left = (leftmost, topmost)\n","upper_right = (rightmost, topmost)\n","lower_right = (rightmost, bottommost)\n","lower_left = (leftmost, bottommost)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"977Ac7XWh9Uh"},"outputs":[],"source":["#Code cell 19\n","#Make a copy of the show_rectangles image with the red rectangles already drawn\n","text_block = show_rectangles.copy()\n","#Draw a rectangle on the image, using the upper_left and lower_right coordinates\n","text_block = cv2.rectangle(text_block, upper_left, lower_right, (255, 255, 0), 3)\n","\n","cv2_imshow(text_block)\n"]},{"cell_type":"markdown","metadata":{"id":"YuxXhm32uN9l"},"source":["####2.d.v - Use imaginary rectangle to crop the original image to the text block\n","It can be hard to remember as we step through the code, but all of these strange-looking white-on-black images are actually representations of our original page image.\n","\n","That means that we can take information derived from these reader-hostile (but computer-friendly) images and apply it to the original image."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kpnhml6w6HeT"},"outputs":[],"source":["#Code cell 20\n","text_block_cropped = cv2color_image.copy()\n","y = topmost\n","x = leftmost\n","w = rightmost\n","h = bottommost\n","text_block_cropped = text_block_cropped[y:h, x:w]\n","cv2_imshow(text_block_cropped)"]},{"cell_type":"markdown","metadata":{"id":"RuuxDn4zyjX9"},"source":["###2.e - Automatically cropping images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b2oPjvJo5Ubf"},"outputs":[],"source":["#Code cell 21\n","def get_text_block(image) :\n","  #Invert\n","  invert = cv2.threshold(cv2blurred_image, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n","  #Floodfill\n","  h, w = invert.shape[:2]\n","  mask = np.zeros((h+2, w+2), np.uint8)\n","  floodfill = invert.copy()\n","  cv2.floodFill(floodfill_image, mask, (0,0), 0);\n","  #Dilate\n","  kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (10, 20))\n","  dilate = cv2.dilate(floodfill, kernel, iterations=5)\n","  #Find_all_contours\n","  contours, hierarchy = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","  #Define middle-of-page area of interest\n","  height = np.shape(dilate)[0]\n","  width = np.shape(dilate)[1]\n","  eighth = int(width/8)\n","  midpoint_x = int(width/2)\n","  middle_zone = (midpoint_x - eighth, midpoint_x + eighth)\n","  #Select contours centered on area of interest\n","  middle_zone_contours = []\n","  for contour in contours :\n","    M = cv2.moments(contour)\n","    contour_x = int(M[\"m10\"] / M[\"m00\"])\n","    if middle_zone[0] <= contour_x <= middle_zone[1] :\n","      middle_zone_contours.append(contour)\n","  #Get bounding rectangles\n","  rectangles = [cv2.boundingRect(contour) for contour in middle_zone_contours]\n","  #Construct text block rectangle\n","  leftx_coords = [rectangle[0] for rectangle in rectangles]\n","  rightx_coords = [rectangle[0] + rectangle[2] for rectangle in rectangles]\n","  topy_coords = [rectangle[1] for rectangle in rectangles]\n","  bottomy_coords = [rectangle[1]  + rectangle[3] for rectangle in rectangles]\n","  leftmost = min(leftx_coords) - 100\n","  rightmost = max(rightx_coords) + 100\n","  topmost = min(topy_coords) - 50\n","  bottommost = max(bottomy_coords) + 50\n","  return image[topmost:bottommost, leftmost:rightmost]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AKz3tlnv5JDB"},"outputs":[],"source":["#Code cell 22\n","if not os.path.exists('/content/cropped/') :\n","  os.makedirs('/content/cropped/')\n","for file in glob.glob('/content/2023_page_images/*.tif') :\n","  basename = os.path.basename(file)[:-4] + '-cropped.tif'\n","  original = cv2.imread(file, cv2.IMREAD_COLOR)\n","  cropped = get_text_block(original)\n","  cv2.imwrite('/content/cropped/' + basename, cropped)\n","  print('Saved ' + basename)"]},{"cell_type":"markdown","metadata":{"id":"N1NNvGz51sn8"},"source":["### Move files out of Colab environment to Google Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jx1Xowt89qcI"},"outputs":[],"source":["#Code cell 23\n","%cd /content/\n","!zip -r cropped.zip cropped/\n","!mv /content/cropped.zip /gdrive/MyDrive/rbs_digital_approaches_2023/output/cropped.zip"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM1q/181PKDaJ1RySg13TEe"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}