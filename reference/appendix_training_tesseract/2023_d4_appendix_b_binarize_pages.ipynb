{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNVcppoVoRBWyvJKmp/xJEm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"4zzZ9Qb92ZVC"},"source":["## Using untrained Tesseract to get line coordinates and preliminary OCR\n","Even without training, Tesseract will do a pretty good job of recognizing lines of text on the page when dealing with print that's more or less like what it's been trained on. Its recognition of the characters isn't what we'd like, but it gets the lines right, at least.\n","\n","To do that, though, we have to prepare the images for OCR, just as we would do if we were actually expecting to get good text out. That starts with binarization."]},{"cell_type":"markdown","metadata":{"id":"_ixgwvM67Jmb"},"source":["## Connect to Google Drive"]},{"cell_type":"code","metadata":{"id":"DvgWEsc42LI8"},"source":["#Code cell #1\n","#Connect to Google Drive\n","from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gEQx6j_u8Iqo"},"source":["## Import packages"]},{"cell_type":"code","metadata":{"id":"0jwpgaOf2md5"},"source":["#Code cell #2\n","#import libraries we'll need\n","import os\n","import glob\n","import cv2\n","from google.colab.patches import cv2_imshow\n","from PIL import Image\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cyo1proK8ma1"},"source":["## Move page images from Google Drive to Colaboratory\n"]},{"cell_type":"code","metadata":{"id":"_YFtK6txexyW"},"source":["#Code cell #3\n","%cp /gdrive/MyDrive/L-100\\ Digital\\ Approaches\\ to\\ Bibliography\\ \\&\\ Book\\ History-2023/penn_pr3732_t7_1730b.zip /content/penn_pr3732_t7_1730b.zip\n","%cd /content/\n","!unzip penn_pr3732_t7_1730b.zip\n","%cd /content/penn_pr3732_t7_1730b/"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"94eY4NKA8BLK"},"source":["## Define functions\n","You've seen all of this code before in prior notebooks. This notebook just repurposes the code from the interactive notebooks into functions that can be called from other cells."]},{"cell_type":"code","metadata":{"id":"wLyLVdWW2nD-"},"source":["#Code cell #4\n","#Define image processing functions: these should look familiar\n","def binarize_invert_color_image(cv2image) :\n","  cv2gray_image = cv2.cvtColor(cv2image, cv2.COLOR_BGR2GRAY)\n","  cv2blurred_image = cv2.GaussianBlur(cv2gray_image, (9, 9), 0)\n","  cv2binary_inverted_image = cv2.threshold(cv2blurred_image, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n","  return cv2binary_inverted_image\n","\n","def get_deskew_angle(cv2image) :\n","  kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (20, 1))\n","  dilate = cv2.dilate(cv2image, kernel, iterations=3)\n","  contours, hierarchy = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","\n","  rects = []\n","  for contour in contours :\n","    minAreaRect = cv2.minAreaRect(contour)\n","    if minAreaRect[1][1] > 60 :\n","      if minAreaRect[-1] not in [0.0, -0.0, 90, -90.0] :\n","        rects.append(minAreaRect)\n","  if len(rects) == 0 :\n","    angle = 0\n","  else :\n","    angle_corrections = []\n","    for rect in rects :\n","      if 45 < rect[-1] < 90 :\n","        angle_corrections.append((-1* (90 - (rect[-1])), -1))\n","      else :\n","        angle_corrections.append((90 - (90 + rect[-1]), 1))\n","    angle = np.mean([angle_tuple[0] for angle_tuple in angle_corrections])\n","    plus_or_minus = sum(angle_tuple[1] for angle_tuple in angle_corrections)\n","    if plus_or_minus > 0 :\n","      angle = -1.0 * angle\n","  return angle\n","\n","def deskew_image(cv2image, angle) :\n","  new_image = cv2image.copy()\n","  (h, w) = new_image.shape[:2]\n","  center = (w // 2, h // 2)\n","  M = cv2.getRotationMatrix2D(center, angle, 1.0)\n","  deskewed_image = cv2.warpAffine(new_image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n","  return deskewed_image\n","\n","def threshold_color_image(cv2image) :\n","  cv2_gray_image = cv2.cvtColor(cv2image, cv2.COLOR_BGR2GRAY)\n","  cv2_blurred_image = cv2.GaussianBlur(cv2_gray_image, (5, 5), 0)\n","  cv2_binary_otsu_image = cv2.threshold(cv2_blurred_image, 0, 255, cv2.THRESH_OTSU)[1]\n","  return cv2_binary_otsu_image\n","\n","def prepare_color_image(image) :\n","  cv2image = cv2.imread(image, cv2.IMREAD_COLOR)\n","  invert = binarize_invert_color_image(cv2image)\n","  deskew_angle = get_deskew_angle(invert)\n","  deskewed = deskew_image(cv2image, deskew_angle)\n","  otsu_image = threshold_color_image(deskewed)\n","  return otsu_image\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ezHmUInCe642"},"source":["## Choose images to work on\n","By default, I've set this notebook to only process a few images, just so you can see how the process flows. If you'd like to process all the images, simply comment out line 4 and uncomment the list from line 5 to line 13."]},{"cell_type":"code","metadata":{"id":"dB2nCuh72q27"},"source":["#Code cell #5\n","source_image_directory = '/content/penn_pr3732_t7_1730b/'\n","source_image_basename = 'PR3732_T7_1730b_body00'\n","source_pages = ['13', '21', '22', '86']\n","# source_pages = ['01', '03', '04', '05', '07', '08', '09', '10', '11',\n","#                 '12', '13', '14', '15', '16', '17', '18', '19', '20', '21',\n","#                 '22', '23', '24', '25', '26', '27', '28', '29', '30', '31',\n","#                 '32', '33', '34', '35', '36', '37', '38', '39', '40', '41',\n","#                 '42', '43', '44', '45', '46', '47', '48', '49', '50', '51',\n","#                 '52', '53', '54', '55', '56', '57', '58', '59', '60', '61',\n","#                 '62', '63', '64', '65', '66', '67', '68', '69', '70', '71',\n","#                 '72', '73', '74', '75', '76', '77', '78', '79', '80', '81',\n","#                 '82', '83', '84', '85', '86']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JRs0qXNkBzJ0"},"source":["## Binarize and deskew pages for OCR\n","This sends each of our color images through the `prepare_color_image` function (which, in turn calls `binarize_invert_color_image`, `get_deskew_angle`, `deskew_image`, and `threshold_color_image`, all defined in code cell #4)."]},{"cell_type":"code","metadata":{"id":"4BWYoo1f2xI6"},"source":["#Code cell #6\n","#Create deskewed black and white derivative files\n","page_output_directory = '/content/penn_pr3732_t7_1730b/bw/'\n","if not os.path.exists(page_output_directory) :\n","  os.makedirs(page_output_directory)\n","for source_page in source_pages :\n","  image = source_image_directory + source_image_basename + source_page + '.tif'\n","  print(image)\n","  outfile_name = page_output_directory + source_image_basename + source_page + '-bw.tif'\n","  bw_deskewed = prepare_color_image(image)\n","  bw_deskewed = Image.fromarray(bw_deskewed)\n","  bw_deskewed.save(outfile_name, dpi=(400,400))\n","  print('Saving ' + outfile_name)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xW65_elRCz0H"},"source":["## Move black and white pages back to Google Drive so we can inspect them more easily\n","If you examine the binarized images, you may well find some where the process we just used didn't yield the best results: perhaps Otsu's method didn't yield the best binarization, or perhaps the deskewing routine didn't quite do the trick for a particular page. (I noticed that page 86 fared pretty badly, for instance, and there may be others I'm missing.)\n","\n","It's much easier to look at images in Google Drive, so we'll compress our folder of black and white images with `zip`, copy that .zip archive over to Google Drive, and then unzip it.\n","\n","When that process is finished, you can view the images in your browser and note any that need to be re-processed. The next notebook gives you a way to tweak any problem images to get a better binarized image to use for preliminary OCR. (Note that, while you have black and white derivatives of *all* of the images, unless you changed the code in code cell #5, you only created new black and white versions of numbers 13, 21, 22, and 86.)"]},{"cell_type":"code","metadata":{"id":"Udjts_QuDwRA"},"source":["#Code cell #7\n","%cd /content/penn_pr3732_t7_1730b/\n","!zip -r penn_pr3732_t7_1730b-bw.zip bw/\n","!mv penn_pr3732_t7_1730b-bw.zip /gdrive/MyDrive/rbs_digital_approaches_2023/output/penn_pr3732_t7_1730b-bw.zip\n","%cd /gdrive/MyDrive/rbs_digital_approaches_2023/output/\n","!unzip penn_pr3732_t7_1730b-bw.zip\n","!mv bw/ penn_pr3732_t7_1730b-bw\n","# !rm penn_pr3732_t7_1730b-bw.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SCQddEa-Jfp0"},"source":["## Wipe out the contents of the Colaboratory environment\n","I don't *think* that leaving all of those .tif files will end up counting against your Google storage quota when you're done, but why take the risk? Let's just delete everything we uploaded here and head back to Google Drive for the next steps."]},{"cell_type":"code","metadata":{"id":"bpSMh5HYJXKx"},"source":["#Code cell #8\n","%cd /content/\n","! rm -r ./*"],"execution_count":null,"outputs":[]}]}