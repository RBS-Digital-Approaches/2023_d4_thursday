{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPNe6vAHiwpYX955ZSckAkk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"YhLdpT6Bb-YK"},"source":["# Using lines from TCP text for Tesseract training ground truth\n","As we saw when we ran our page images through Tesseract to get coordinates for the line images in hOCR XML, Tesseract *does* recognize text in these page images, it's just that the accuracy isn't what we'd like.\n","\n","In this notebook, we're going to compare Tesseract's output to the lines we've extracted from the modified version of the ECCO-TCP transcription of *Sophonisba*."]},{"cell_type":"code","metadata":{"id":"GPxuxUMD-AxG"},"source":["#Code cell #1\n","#Connect to Google Drive\n","from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3V-I3ngjxe_6"},"source":["## 1 - Install Python package for calculating Levenshtein distance and import needed packages\n","In comparing the OCR output from our untrained installation of Tesseract to the lines of the transcribed (and corrected) text, we'll use a measure called [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance), which calculates the difference between two strings: basically, how many changes would we have to make to the first string to turn it into the other."]},{"cell_type":"code","metadata":{"id":"22ymjUG0HpB0"},"source":["#Code cell #2\n","!pip install python-Levenshtein"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5O4fY3nqFTZY"},"source":["#Code cell #3\n","import os\n","import glob\n","from bs4 import BeautifulSoup\n","import lxml\n","import re\n","import Levenshtein"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JGZhTPuF8aYd"},"source":["### 1.a - A quick look at Levenshtein distance\n","Let's take a quick look at Levenshtein distance in action."]},{"cell_type":"code","metadata":{"id":"LP1bSVOu-AKu"},"source":["#Code cell #4\n","def show_lev(list) :\n","  lev_distance = Levenshtein.distance(list[0], list[1])\n","  string = str(lev_distance)\n","  return 'Levenshtein distance between \"' + list[0]  + '\" and \"' + list[1] + '\" is: ' + string\n","\n","first = show_lev(['slip', 'slop'])\n","second = show_lev(['sister', 'sinister'])\n","third = show_lev(['I can see clearly now', 'I will pay dearly now'])\n","print(first)\n","print(second)\n","print(third)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8w5OVYxoCqFk"},"source":["## 2 - Copy files from Google Drive to Colaboratory environment and set paths\n","To reduce the amount of input/output between Colab and Google Drive, we'll move files into the Colab environment to work on. In this notebook, I'm going to assume you're working with the pre-prepared materials I provided in the shared Google Drive folder for our class. If you'd like to use the (same) materials that you created in the prior notebooks, comment out lines 1 and 8 and uncomment the following lines."]},{"cell_type":"code","metadata":{"id":"7Q3Bgd8TWS-u"},"source":["#Code cell #5\n","!mkdir /content/ocr_training_materials/\n","!cp /gdrive/MyDrive/rbs_digital_approaches_2023/output/hocr.zip /content/ocr_training_materials/hocr.zip\n","!cp /gdrive/MyDrive/rbs_digital_approaches_2023/output/tcp_lines.zip /content/ocr_training_materials/tcp_lines.zip\n","!cp /gdrive/MyDrive/rbs_digital_approaches_2023/output/line_images.zip /content/ocr_training_materials/line_images.zip\n","%cd /content/ocr_training_materials/\n","!unzip hocr.zip\n","!unzip tcp_lines.zip\n","!unzip line_images.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-UkYlqwmFUWs"},"source":["#Code cell #6\n","hocr_directory = '/content/ocr_training_materials/hocr/'\n","tcp_line_directory = '/content/ocr_training_materials/tcp_lines/'\n","line_image_directory = '/content/ocr_training_materials/line_images/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k7trEBTXc1Ig"},"source":["## 3 - Compare Tesseract output from hOCR files to lines from TCP text and try to match them"]},{"cell_type":"markdown","metadata":{"id":"o8JxmeHRdAwr"},"source":["### 3.a - Get each Tesseract page\n","This reads the hOCR files and extracts the text content of the OCR'ed lines. The `tesseract_pages` dictionary has page number as keys and lists as values: for each page, there is a list of lines that we get from the hOCR files."]},{"cell_type":"code","metadata":{"id":"idCqLRJoIz4x"},"source":["#Code cell #7\n","#Create dictionary\n","tesseract_pages = {}\n","\n","#Begin processing hOCR files\n","for hocr_file in glob.glob(hocr_directory + '*.xml') :\n","  #Mangle filenames to get identifiers\n","  filename = os.path.split(hocr_file)[1]\n","  basename = filename[:filename.rfind('-')]\n","  page = int(basename[basename.rfind('_')+5:].lstrip('0'))\n","\n","  #Don't need these\n","  if page not in ['1', '2'] :\n","\n","    #Create an entry for this page in the dictionary\n","    tesseract_pages.setdefault(page, [])\n","\n","    #Open the file and get ocr_lines\n","    with open(hocr_file, 'r') as hocr :\n","      file_read = hocr.read()\n","      soup = BeautifulSoup(file_read, 'xml')\n","      lines = soup.find_all('span', class_='ocr_line')\n","\n","      #Set an identifier for line numbers\n","      i = 1\n","\n","      for line in lines :\n","        #Strip the line break character\n","        recognized_line = line.get_text().replace('\\n', ' ')\n","        #Append a new item to the list of lines for this page.\n","        #Each recognized line is presented as a tuple with an integer\n","        #for the line number and the text as recognized by Tesseract\n","        tesseract_pages[page].append((i, recognized_line))\n","\n","        #Increment the counter for our line number identifier\n","        i += 1\n","\n","#See what we have\n","for k, v in tesseract_pages.items() :\n","  print(k)\n","  for content in v :\n","    print(content)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6VQ1Y-a-dEk6"},"source":["### 3.b - Get each TCP page\n","We're going to more or less repeat what we did for Tesseract's OCR with the lines we identified from the TCP text."]},{"cell_type":"code","metadata":{"id":"bLcopAeucvvS"},"source":["#Code cell #8\n","tcp_pages = {}\n","\n","for tcp_line_file in glob.glob(tcp_line_directory + '*.txt') :\n","  filename = os.path.split(tcp_line_file)[1]\n","  page_num = int(filename.rstrip('.txt'))\n","  tcp_pages.setdefault(page_num, [])\n","  i = 1\n","  with open(tcp_line_file, 'r') as tcp_line :\n","    clean_text = tcp_line.readlines()\n","    for line in clean_text:\n","      tcp_pages[page_num].append((i, line.rstrip('\\n')))\n","      i += 1\n","\n","for k, v in sorted(tcp_pages.items()) :\n","  print(k)\n","  for entry in v :\n","    print(entry)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5fHEUIOldIYX"},"source":["### 3.c - Get Levenshtein distance and accept corrections (or not)"]},{"cell_type":"code","metadata":{"id":"p2d8BGi8cl1G"},"source":["#Code cell #10\n","#I know there's a built-in punctuation list, but couldn't remember how to use it\n","#when I was writing this\n","punct = re.compile(r'[\\!@#\\$\\%\\^&\\*\\(\\)\\-_\\+=\\{\\}\\[\\]\\|\\\\\\:;\\\"\\'\\\\’<\\>,\\.\\?\\/]')\n","emdash = re.compile(r'—')\n","replacements = {}\n","\n","for tesseract_k, tesseract_v in tesseract_pages.items() :\n","  if tesseract_k > 2 :\n","    #Setting default value for all replacements to say \"-NO_MATCH.\" Seems kind of\n","    #pessimistic, in retrospect\n","    for i in enumerate(tesseract_v) :\n","        label = str(tesseract_k) + '-' + str(tesseract_v[i[0]][0])\n","        replacements.setdefault(label, '-' + tesseract_v[i[0]][1] + '-NO_MATCH')\n","\n","    #TEI doesn't have any forme work. Need to figure out what to do with\n","    #running title and signature/catchword lines: could save them explicitly\n","    #marked as not attempted to match? Not doing that here. Just ignoring first\n","    #and last lines in tesseract output, hence [1:-1]\n","    for tesseract_index, entry in enumerate(tesseract_v[1:-1]) :\n","        tesseract_line_num = entry[0]\n","        tesseract_ocr = entry[1]\n","        label = str(tesseract_k) + '-' + str(tesseract_line_num)\n","\n","        #Get rid of punctuation for purposes of comparison—seems to be lots of\n","        #spurious punctuation in recognized text which causes otherwise good\n","        #to fail\n","        no_punct_tesseract = re.sub(punct, '', tesseract_ocr).strip()\n","\n","        #Work through TCP lines from page corresponding to Tesseract page (off by 1)\n","        for tcp_v in sorted(tcp_pages[tesseract_k-1]) :\n","            tcp_line_num = tcp_v[0]\n","            transcribed_text = tcp_v[1]\n","\n","            #Eliminate punctuation for matching of word (see above)\n","            no_punct_transcribed = re.sub(punct, '', transcribed_text)\n","\n","            #Indices of lines need to be within 5 of each other so we're not\n","            #comparing a line at the top of the Tesseract page to something\n","            #way down the TCP page\n","            if -5 <= tcp_line_num - tesseract_index <= 5 :\n","\n","                #If there's an exact match, then heck yeah, let's accept it\n","                if no_punct_tesseract == no_punct_transcribed :\n","                    replacements[label] = (tesseract_k-1, tcp_line_num, transcribed_text)\n","                else :\n","                    #Short lines need a different threshold for what Levenshtein\n","                    #distance indicates a probable match represents\n","                    if len(no_punct_transcribed) < 20 :\n","\n","                        #For method for determining Levenshtein distance threshold\n","                        #in this code, please see:\n","                        #https://en.wikipedia.org/wiki/Scientific_wild-ass_guess#Use\n","                        if -0.25 <= ((len(no_punct_transcribed) - len(no_punct_tesseract)) \\\n","                                     / len(no_punct_transcribed)) <= 0.25 :\n","                            lev_dist = Levenshtein.distance(no_punct_tesseract, no_punct_transcribed)\n","                            if lev_dist / len(no_punct_transcribed) < 0.4 :\n","                                replacements[label] = (tesseract_k-1, tcp_line_num, transcribed_text)\n","                    else :\n","                        if -0.10 <= ((len(no_punct_transcribed) - len(no_punct_tesseract)) \\\n","                                     / len(no_punct_transcribed)) <= 0.10 :\n","                            lev_dist = Levenshtein.distance(no_punct_tesseract, no_punct_transcribed)\n","                            if lev_dist / len(no_punct_transcribed) < 0.4 :\n","                                replacements[label] = (tesseract_k-1, tcp_line_num, transcribed_text)\n","\n","for orig, repl in replacements.items() :\n","  print(orig, repl)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5w-hkmFYNjiZ"},"source":["### 3.d - How did we do?\n","Let's compare the number of Tesseract lines to the number we were able to match"]},{"cell_type":"code","metadata":{"id":"y0Swet4xNqvl"},"source":["#Code cell #11\n","num_tesseract_lines = 0\n","for tesseract_key, tesseract_value in tesseract_pages.items() :\n","  for line in tesseract_value :\n","    num_tesseract_lines += 1\n","\n","num_replacements = 0\n","for orig, repl in replacements.items() :\n","  if repl[2].find('-NO_MATCH') == -1 :\n","    num_replacements += 1\n","\n","print(num_tesseract_lines, num_replacements)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zH-06_n9OSD9"},"source":["I can accept that."]},{"cell_type":"markdown","metadata":{"id":"iWLA_xhVdUdz"},"source":["## 4 - Save accepted corrections as individual text files\n","We now need to save each of the replacements we identified"]},{"cell_type":"code","metadata":{"id":"WTx_0qaZolpK"},"source":["#Code ccell #12\n","filename_base = 'Penn_PR3732_T7_1730b-'\n","\n","#I'm violating our class naming conventions by using hyphens rather than\n","#underscores in the directory name because the script we'll use to train\n","#Tesseract expects the folder to be named that way\n","groundtruth_directory = '/content/ocr_training_materials/sophonisba-ground-truth/'\n","if not os.path.exists(groundtruth_directory) :\n","  os.makedirs(groundtruth_directory)\n","  print('Creating directory')\n","a = 0\n","for orig, replacement in replacements.items() :\n","  if isinstance(replacement, tuple) :\n","    page = orig[:orig.find('-')]\n","    #zeropadding because I stupidly stripped zeroes out in an earlier step\n","    page_number = page.zfill(4)\n","    line = orig[orig.rfind('-')+1:]\n","\n","    filename = filename_base + page_number + '-line-' + str(line)\n","    # print(filename)\n","    # print(filename + '.txt' + ' | ' + replacement[2])\n","\n","    with open(groundtruth_directory + filename + '.gt.txt', 'w') as groundtruth_line_out :\n","      print('Writing ' + groundtruth_directory + filename + '.gt.txt...')\n","      groundtruth_line_out.write(replacement[2])\n","      a += 1\n","print(str(a) + ' files saved')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AfnlOna-PPIx"},"source":["I'm not quite sure why this ends up writing about 500 fewer text line files than we have matches. I'll have to try to track this down another day..."]},{"cell_type":"markdown","metadata":{"id":"CYVxofBzkAF-"},"source":["## 5 - Get line image files for every matched ground truth line"]},{"cell_type":"code","metadata":{"id":"GT12YBBnxJJr"},"source":["#Code cell #13\n","z = 1\n","for file in glob.glob(groundtruth_directory + '*.gt.txt') :\n","  filename = os.path.split(file)[1]\n","  basename = filename[:filename.rfind('.gt.txt')]\n","\n","  if os.path.exists(line_image_directory + basename + '.tif') :\n","    print('Moving ' + line_image_directory + basename + '.tif to ' + groundtruth_directory)\n","    os.rename(line_image_directory + basename + '.tif', groundtruth_directory + basename + '.tif')\n","    z += 1\n","\n","print(str(z) + ' image files moved')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"knSRO9oqmzy6"},"source":["## 6 - Have a look at our ground truth folder\n","We now have pairs of .tif files and .gt.txt files for a great many lines of text. Not every line, of course (since there were some that we couldn't match using our little Levenshtein distance trick above), but a lot of lines, nonetheless.\n","\n","We need to make sure that all of these files come in pairs—that is, that for every .gt.txt file there's a corresponding .tif file. We just brought in the .tif files where we could match them to a .txt file, but let's take a pass through and see if we have any .txt files that *didn't* find a matching .tif file.\n","\n","If all goes to plan, code cell #14 should print out 0. If you get anything other than that, run code cell #15."]},{"cell_type":"code","metadata":{"id":"P-K5SShmDb9x"},"source":["#Code cell #14\n","partnerless = []\n","for textfile in glob.glob('/content/ocr_training_materials/sophonisba_ground_truth/*.txt') :\n","  basename = os.path.split(textfile)[1].rstrip('.gt.txt')\n","  if os.path.exists('/content/ocr_training_materials/sophonisba_ground_truth/'\n","                    + basename + '.tif') is not True :\n","    partnerless.append(basename)\n","print(len(partnerless))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wN0RJgzCJIOe"},"source":["#Code cell #15\n","for lonefile in partnerless :\n","  os.remove('/content/ocr_training_materials/sophonisba_ground_truth/'\n","            + lonefile + '.gt.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sFpUbpzX59sw"},"source":["For our purposes, we're going to stop here and hope that this gives us enough to work with for training Tesseract.\n","\n","As we'll see later, the lines we collected here don't include every character we could possibly want for recognzing eighteenth-century print—there's no upper-case Z in *Sophonisba*, for example, and the text we have is light on numerals (we could perhaps correct that latter point by going back to get the page numbers in the running titles). If we were trying to do a large-scale OCR training, our time would be more profitably spent processing images of more texts than it would be going back and trying to chase down every line we missed."]},{"cell_type":"markdown","metadata":{"id":"GTa8OWPZhOL9"},"source":["## 7 - Compress folder of ground truth text lines and save to Google Drive"]},{"cell_type":"code","metadata":{"id":"fSzUXu4zhTlG"},"source":["#Code cell #16\n","%cd /content/ocr_training_materials/\n","!zip -r sophonisba-ground-truth.zip sophonisba-ground-truth/\n","!mv sophonisba-ground-truth.zip /gdrive/MyDrive/rbs_digital_approaches_2023/output/ocr_training_materials/sophonisba-ground-truth.zip\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8bQ_jmfDit4e"},"source":["## 8 - Clear Colaboratory environment"]},{"cell_type":"code","metadata":{"id":"Gz_t8Hv0iwZ6"},"source":["#Code cell #17\n","%cd /content/\n","! rm -r ./*"],"execution_count":null,"outputs":[]}]}