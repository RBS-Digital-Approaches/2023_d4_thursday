{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNAbE/TWQ10Q4LRbuhxnDZ0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"DCEKnhaIQVu2"},"source":["# Build an eighteenth-century word list for Tesseract\n","We're mostly trying to train Tesseract to recognize eighteenth-century letter forms, but we can improve Tesseract's chances of getting our text right if we also provide it with some information about the kinds of words (and punctuation) we expect to find in the kinds of text that we're hoping to use Tesseract to recognize.\n","\n","In this notebook, we'll plunder the ECCO-TCP corpus for words to use as a dictionary for training. ECCO-TCP is *much* smaller than EEBO-TCP, but it's big enough to find words that aren't in Tesseract's default English language model."]},{"cell_type":"markdown","metadata":{"id":"7vksjDEhSin0"},"source":["## 1 - Connect to Google Drive and import packages"]},{"cell_type":"code","metadata":{"id":"i0QRH92sTsjh"},"source":["#Code cell #1\n","#Connect to Google Drive\n","from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-IV0FzX8UsZF"},"source":["#Code cell #2\n","import os\n","import glob\n","import shutil\n","from bs4 import BeautifulSoup\n","import lxml\n","import nltk"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zZWMtFqKRUCe"},"source":["## 2 - Move files from Google Drive to Colaboratory\n","The files available from the TCP GitHub repository don't have the long-s characters in the transcriptions. I happen to have on my hard drive what I believe must be an earlier release of the texts that does have it. (It may be that it was just from a different source: the versions at the [Oxford Text Archive](http://www.ota.ox.ac.uk/) do have them, and I rather suspect this may have been where I got them, at a point when they had a system for bulk download). We'll use my copy, which I've placed in the shared Google Drive folder for the class, rather than on GitHub."]},{"cell_type":"code","metadata":{"id":"fnpAfRRwUGUq"},"source":["#Code cell #3\n","%cp /gdrive/MyDrive/L-100a/ecco_tcp.zip /content/ecco_tcp.zip\n","%cd /content/\n","!unzip ecco_tcp.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YmNVjU7Fb7Q1"},"source":["## 3 - Extract plain text from the TEI files\n","For the purposes of generating a word list, we don't need any of the TEI markup that's in the TCP texts. This cell uses Beautiful Soup to extract the text content from each file and save it as a plain text file. (Note: this will take several minutes.)"]},{"cell_type":"code","metadata":{"id":"nfbn18LrUyXC"},"source":["#Code cell #4\n","#Save plaintext versions of ECCO-TCP texts\n","corpus_directory = '/content/ecco_tcp/plain_text/'\n","if not os.path.exists(corpus_directory) :\n","  os.makedirs(corpus_directory)\n","for filepath in glob.glob('/content/ecco_tcp/*.xml') :\n","  filename = os.path.basename(filepath)[:-4]\n","  # print(filename)\n","  with open(filepath, 'r') as infile :\n","    content = infile.read()\n","    soup = BeautifulSoup(content, 'xml')\n","    text = soup.find('text').get_text()\n","    #Causes problems due to concatenation of end-of-line-hyphenated words??\n","    # text = text.replace('∣','')\n","  with open(corpus_directory + filename + '.txt', 'w') as outfile :\n","    outfile.write(text)\n","    print('Saved ' + filename + '.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vwRnu1d_UsCo"},"source":["## 4 - Get distinct words\n","There may well be other ways to do this—this is a question I should have asked Carl on text mining day. I used `nltk` to build a corpus from the plain text versions of the ECCO-TCP texts, then ran a frequency distribution to get unique tokens in the corpus."]},{"cell_type":"code","metadata":{"id":"75nTojZKYfzA"},"source":["#Code cell #5\n","corpusdir = '/content/ecco_tcp/plain_text/'\n","from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n","ecco_tcp_eng_corpus = PlaintextCorpusReader(corpusdir, '.*')\n","print(len(ecco_tcp_eng_corpus.words()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ux-4a7TFtFlA"},"source":["#Code cell #6\n","from nltk import FreqDist\n","fdist = FreqDist(ecco_tcp_eng_corpus.words())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zew8DiHXtIMv"},"source":["#Code cell #7\n","print(len(fdist.keys()))\n","tokens = [key for key in fdist.keys()]\n","tokens.sort()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RWINy6VJVPBp"},"source":["That seems like a lot of distinct words. The fact that I have to get past the 10,000th token just to get the \"A\"s gives me pause."]},{"cell_type":"code","metadata":{"id":"3MfPPXXbwIF4"},"source":["#Code cell #8\n","for token in tokens[10500:10550] :\n","  print(token)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_jU0sny2Vies"},"source":["I worked up a few regular expressions to try to trim away some of the noise..."]},{"cell_type":"code","metadata":{"id":"pmcxtAWvzVh3"},"source":["#Code cell #9\n","import re\n","#I know there are defined lists of punctuation. But there's all *sorts* of freaky\n","#stuff in the ECCO-TCP texts...\n","punct_pattern = re.compile(r'^[!@#$%\\^&\\*\\(\\)\\-_\\+=\\{\\}\\[\\]\\|\\\\;\\:\\'\\\"\\\"<,>\\.\\?\\/€‹›ﬂ‡°·—±„´ˇÁ¨\\\"\\'»Ó˝◊¿▪.…☜☞]*$')\n","\n","#Anything that, from beginning to end, is composed of characters that are not\n","#alphabetic\n","nonword = re.compile(r'^[^A-Za-z]+$')\n","\n","#I don't have anything against numbers. I just don't want them at the beginning\n","#of my words. I mean, it's okay in the titles of Prince songs, I guess...\n","contaminated = re.compile(r'^[0-9]+')\n","\n","#Anything that's only lettes (including long-s)\n","all_alphabetic = re.compile(r'^[A-Za-zſ]+$')\n","\n","#Define some lists to hold results\n","punct = []\n","words = []\n","\n","#Search for matches of these regular expressions, and add them to the lists\n","for token in tokens :\n","  if re.match(punct_pattern, token) is not None :\n","    punct.append(token)\n","  if re.match(all_alphabetic, token) is not None:\n","    words.append(token)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iaohkueikhux"},"source":["There were still plenty of problems with my word list, but after trying a few things that were simply taking too long on Colaboratory (or possibly in Python), I exported the word list as-is and processed it some more in a matter of minutes in my text editor, reducing a 5.1MB file to 3.8MB file (which is still more than 400,000 words). I've included those files in the collection of pre-prepared materials that you can use for the last notebook in this sequence."]},{"cell_type":"code","metadata":{"id":"E9c5JZX6ajiD"},"source":["#Code cell #10\n","with open('/content/ecco-words.txt', 'w') as wordfile :\n","  for word in words :\n","    wordfile.write(word + '\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mGxHLqlJcF3j"},"source":["#Code cell #11\n","with open('/content/ecco-punct.txt', 'w') as punctfile :\n","  for pattern in punct :\n","    punctfile.write(pattern + '\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hJRg23c2m2EN"},"source":["#Code cell #12\n","%cd /content/\n","!zip training_lists.zip *.txt\n","!mv training_lists.zip /gdrive/MyDrive/rbs_digital_approaches_2023/output/ocr_training_materials/training_lists.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mdpKYwaElADE"},"source":["## 4 - Clear Colaboratory environment"]},{"cell_type":"code","metadata":{"id":"ctW_5UwOlCoJ"},"source":["#Code cell #13\n","%cd /content/\n","! rm -r ./*"],"execution_count":null,"outputs":[]}]}