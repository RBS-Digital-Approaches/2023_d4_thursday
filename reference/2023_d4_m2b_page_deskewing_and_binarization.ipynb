{"cells":[{"cell_type":"markdown","metadata":{"id":"wh2j0fLvij2q"},"source":["# Deskewing pages\n","This notebook was designed to introduce one method for automatically deskewing page images in preparation for OCR. For our purposes in this iteration of the class, the details of the process are less important than seeing an example of the ways that images can be computationally changed after initial photography: when you're looking at a digital surrogate, you're seeing an image that has likely been through several processes that attempt to optimize it for the task at hand without a human having to check it at each step of the way.\n","\n","For today, there's **really** no need to try to figure out any of the details of the code: the more important thing is to get a general sense of what's happening, and then observe the differences that those changes make to the image in the end.\n","\n","The code in this notebook is drawn from a blog post by Leo Ertuna at [Becoming Human](https://becominghuman.ai/how-to-automatically-deskew-straighten-a-text-image-using-opencv-a0c30aed83df), but with an adjustment that some experimenting suggests seems to work better for early print."]},{"cell_type":"markdown","metadata":{"id":"EeYTN8gDMyb9"},"source":["### A note before we start: this may not be the only problem to solve\n","The code in this notebook assumes that the problem with the page image is that it's skewed and needs to straightenedâ€”this treats the page as a two-dimensional plane.\n","\n","That works pretty well if the images are of reasonably flat pages of the kind that we can often get from the sort of imaging labs that many libraries have. But books don't necessarily lay flat, and, depending on the condition of the binding, it may not always possible to flatten the pages for imaging. So the lines of text in some images will appear not just skewed, but actually curved, due to the curvature of the pages. And pages in a book can curl in a number of ways all at once (recall, for instance how much more and how differently the pages in the middle of a thick book curl compared to pages at the beginning or end.)\n","\n","It's possible to reduce or eliminate the appearance of curvature in the lines of a page image incorporating some of techniques we'll see in this deskewing routine (but adding some others). That's a more complicated problem that we won't take on, but there's a great [blog post by Mark Zucker](https://mzucker.github.io/2016/08/15/page-dewarping.html) that walks through a solution. The blog post offers visualizations of what's happening at each step, so it's an instructive read even if you're not examining the details of the code. In the interests of time, I'll urge you **not** to examine the details of the code today, but to skim Zucker's description of his approach to the problem and look at the illustrations that visualize what the code is *doing*."]},{"cell_type":"markdown","metadata":{"id":"xhhbM2leRoE2"},"source":["## 1 - Connect to Google Drive, copy files, and install packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vr2ZLKaBb8Kc"},"outputs":[],"source":["#Code cell #1\n","#Get access to Google Drive\n","from google.colab import drive\n","drive.mount('/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kM_B4qhhc4bp"},"outputs":[],"source":["#Code cell #2\n","%cp -r /gdrive/MyDrive/rbs_digital_approaches_2023/output/cropped.zip /content/cropped.zip\n","%cd /content/\n","!unzip cropped.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h5kdzNrKcbUL"},"outputs":[],"source":["#Code cell #3\n","#Install IPyWidgets to provide widgets for experimenting with some variables later\n","import ipywidgets as widgets\n","from ipywidgets import interact\n","\n","#Import necessary Python packages for use in our code.\n","\n","#Note that opencv-python is installed by default in Google Colaboratory. If you\n","#were working in a different environment, you'd need to be sure it was installed\n","#using pip\n","\n","#(The second import is specific to Google Colaboratory and provides a workaround\n","#to get OpenCV's imshow command to work properly in a Colab notebook.\n","import cv2\n","from google.colab.patches import cv2_imshow\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"aO7EsJLZR5K6"},"source":["## 2 - Opening the image"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"TAxKrLvwC6zw"},"outputs":[],"source":["#@title Select an image\n"," #@markdown Run this cell to create a select\n"," #@markdown list widget that allows us to\n"," #@markdown choose an image to process. With\n"," #@markdown an image selected (a default is\n"," #@markdown provided), you can continue\n"," #@markdown working through the code below.\n"," #@markdown FYI: 1730f_p0iv, 1730g_p21, and\n"," #@markdown 1730j_p21 are the best examples\n"," #@markdown of skewed pages in this set.\n","\n"," #@markdown You only need to run this cell once (re-running it will just set things back to the default value). But you can change the image you're working with using the select list in order to see how these processes work given different starting images.\n","\n"," #@markdown >Note: `1730f_p14-cropped.tif` is here to show a nasty surprise! Try some others first.\n","import os\n","import glob\n","file_list = sorted([os.path.basename(file) for file in glob.glob('/content/cropped/*.tif')])\n","image_select = widgets.Dropdown(\n","    description='Choose image',\\\n","    options = file_list,\\\n","    value = '1730f_p0iv-cropped.tif',\n","    style={'description_width': 'initial'})\n","display(image_select)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rWyifXGpdPDR"},"outputs":[],"source":["#Code cell #5\n","#Identify the skewed image and have OpenCV read it. (This can take a little\n","#while, so give it time to complete.)\n","\n","source_directory = '/content/cropped/'\n","skewed_image = source_directory + image_select.value\n","im = cv2.imread(skewed_image, cv2.IMREAD_COLOR)\n","#Let's see what the image looks like: an excellent image, but a little skewed.\n","cv2_imshow(im)"]},{"cell_type":"markdown","metadata":{"id":"S22E8xx2n42d"},"source":["## 3 - Deskewing the image\n","OCR software like Tesseract might well be able to handle an image like this, but recognition of text lines will be better if we can straighten it. Ertuna's script offers a nice example of a workflow for figuring out exactly *how* skewed the image is, then using that measurement to straighten the image. As we proceed, you'll see some of the ways that images that are good for *us* are not as useful for the computer, and vice versa.\n","\n","A lot of what we do in this notebook will look familiar from our cropping procedure, but we'll see there are some slight differences, too."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JLPMgkleaPEW"},"outputs":[],"source":["#Code cell #6\n","#Make a copy of the image\n","newImage = im.copy()\n","#Convery to grayscale\n","gray = cv2.cvtColor(newImage, cv2.COLOR_BGR2GRAY)\n","#Apply a Gaussian blur to reduce the effect of any noise in the image\n","blur = cv2.GaussianBlur(gray, (9, 9), 0)\n","#Convert the image to inverted black and white (i.e., white text on a\n","#black background). Note that Ertuna's script uses Otsu's method for\n","#thresholding to black and white.\n","thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n","cv2_imshow(thresh)"]},{"cell_type":"markdown","metadata":{"id":"xplCmGRlS4J3"},"source":["### 3.a - Set kernel size for dilation\n","Run the next cell to create a few sliders that will allow you to adjust the variables used in the next steps. (You only need to run that cell once. Thereafter, changing the sliders will change the values of the variables used in the subsequent cells.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WNEI3nnFnG_7"},"outputs":[],"source":["#@title Set kernel size {display-mode: \"form\"}\n","\n","#@markdown Run the code in this cell to create a set of\n","#@markdown slider widgets for changing the values of the\n","#@markdown \"kernel\" used to dilate the white pixels in the\n","#@markdown image. You can change the height and width of the\n","#@markdown kernel (i.e., the amount of vertical and horizontal\n","#@markdown dilation to be applied) as well as the number of\n","#@markdown iterations (how many times the dilation operation\n","#@markdown will be applied.)\n","\n","#@markdown You only need to run this cell once (re-running\n","#@markdown it will just re-set the values to their defaults).\n","#@markdown You can change the values of the sliders and\n","#@markdown then run Code cell 12 to see the different\n","#@markdown effects that different values have.\n","kernel_width = widgets.IntSlider(description = 'Kernel width', \\\n","                                               min=1, max=25, step=1, value=20)\n","kernel_height = widgets.IntSlider(description='Kernel height', \\\n","                                                 min=1, max=25, step=1, value=1)\n","num_iterations = widgets.IntSlider(description='Iterations', min=1, \\\n","                      max=10, step=1, value=3)\n","display(kernel_width)\n","display(kernel_height)\n","display(num_iterations)"]},{"cell_type":"markdown","metadata":{"id":"u7avDI3XfOOJ"},"source":["### 3.b - This is weird, but we've seen this before\n","Just as we did when we were trying to identify the text block to crop, we're going to create recognizable text *regions* by dilating white pixels of the text until they run together to form solid blocks of white. The sliders are set to default values that will tend to do this, but you should play around with them to see how the image responds to different settings."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ga-SDNWCakMh"},"outputs":[],"source":["#Code cell #7\n","#The kernel variable defines a shape to use for dilating the pixels. If the kernel\n","#is wider than it is tall, the text will tend to run together while more or less\n","#maintaining the vertical dimensions of the text lines.\n","kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_width.value, kernel_height.value))\n","\n","dilate = cv2.dilate(thresh, kernel, iterations=num_iterations.value)\n","cv2_imshow(dilate)"]},{"cell_type":"markdown","metadata":{"id":"43N1xuI7g7Op"},"source":["This cell finds the boundaries of the dilated white blocks that used to be our text lines and determines their contours.\n","I've made an adjustment to Ertuna's approach here in using the `RETR_EXTERNAL` method rather than the `RETR_LIST` method that he used. Ertuna's method retrieves *all* contours that are detected, where `RETR_EXTERNAL` ignores contours that are found *within other contours*. Though I can't say I've tested it entirely systematically, this approach seems to do a better job of detecting text blocks in this eighteenth-century text, where what may be wandering in the baseline of the set type creates some gaps that end up being detected as contours."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dEyqVb6ha5VB"},"outputs":[],"source":["#Code cell #8\n","#Determine contours\n","contours, hierarchy = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","\n","#These next steps are not really part of the deskewing sequence. I've included\n","#them simply so we can see what's happening. We first create a color version\n","#of our black-and-white image, then draw a bright green line connecting the\n","#contour points so we can see the outlines of the detected shapes\n","show_contours = cv2.cvtColor(dilate, cv2.COLOR_BayerGR2RGB)\n","show_contours = cv2.drawContours(show_contours, contours, -1, (115,255,105), 3)\n","cv2_imshow(show_contours)"]},{"cell_type":"markdown","metadata":{"id":"Uy5WJ4ROiavU"},"source":["### 3.c - Finding the rectangles that fit these contours\n","In the last notebook, we used `boundingRect` to find *straight* bounding rectangles around the text contours. Our problem is slightly different here, since we're interested in lines that *aren't* straight. Indeed, how far they are from being straight is exactly what we want to know.\n","\n","OpenCV's `minAreaRect` finds the *smallest possible* rectangle that will contain the contour, even if that that smallest possible rectangle is rotated relative to the square edges of the image (as it almost surely will be).\n","\n","This really seems like a case where a picture is worth a thousand words, so here's a picture taken from the OpenCV documentation. (The green rectangle is produced by `boundingRect` while the red one is produced by `minAreaRect`.)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9aZCIgtps07l"},"source":["![image.png](data:image/png;base64,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)"]},{"cell_type":"markdown","metadata":{"id":"O6NHzoras9j1"},"source":["`minAreaRect` is the right choice for this de-skewing problem because it provides:\n","\n","* the x, y coordinates of the center point of the rectangle;\n","* the width and height of the rectangle; and\n","* the *rotation angle* of the rectangleâ€”which is what we're trying to correct for. (For more on how `minAreaRect` treats this angle, see [this post at *The AI Learner*](https://theailearner.com/tag/cv2-minarearect/).)"]},{"cell_type":"markdown","metadata":{"id":"mkkz_e_Ckh5V"},"source":["Ertuna's script acts on only the largest of the detected areas on the not-unreasonable premise that the skew angle of the largest text block will be a good proxy for the skew angle of the entire page of text. (He notes, though, that other approaches are possible. One might find that the angle of a different block yielded better results, or the average of multiple blocks.) This seems like it would be a pretty sound approach if we were dealing with deskewing, say, a page printed from a laser printer that had subsequently been scanned with the paper skewed slightly on a scanner bed.\n","\n","But while this approach seemed to work well for images from a copy of *Sophonisba* from the University of Pennsylvania that were significantly skewed, it actually seemed to make things slightly *worse* for some pages that were only minimally skewed. I'm not entirely sure what to make of this yet, but my hunch is that this result may arise from the character of pages produced on a printing press. Perhaps if the chase wasn't locked up tightly, different lines on the same page could deviate from horizontal by *different* amounts?\n","\n","Rather than relying solely on the largest text block, the code below sets a couple of thresholds to detect as many text blocks as possible (while rejecting regions that seem like they're probably noise), then figures out an average angle for deskewing the entire page."]},{"cell_type":"markdown","metadata":{"id":"8ZrcIb8St4VJ"},"source":[">*Note:* The code in the following cell is, again, not strictly part of the deskewing procedure, but I have added it to show what's happening."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o1tU-RbJxJqw"},"outputs":[],"source":["#Code cell #9\n","def draw_min_area_rect(cv2minimumarearectangle, base_image) :\n","  draw_min_area_rect = base_image.copy()\n","  #If we have more than one recttangle...\n","  if isinstance(cv2minimumarearectangle, list) == True :\n","    print(len(cv2minimumarearectangle))\n","    for rect in cv2minimumarearectangle :\n","      #boxPoints gets the coordinates of the four corners of the rotated rectangle,\n","      #which is nice, because figuring them out ourselves would be sort of a drag,\n","      #given the way those rectangles are (of necessity) represented in OpenCV.\n","      min_area_box = cv2.boxPoints(rect)\n","      #Turn corner coordinates into integers\n","      min_area_box = np.int0(min_area_box)\n","      #Draw this rectangle one side at a time, beginning from the upper left\n","      #corner and working clockwise.\n","      draw_min_area_rect = cv2.line(draw_min_area_rect, (min_area_box[0][0], min_area_box[0][1]), \\\n","                                    (min_area_box[1][0], min_area_box[1][1]), (0, 30, 255), 3)\n","      draw_min_area_rect = cv2.line(draw_min_area_rect, (min_area_box[1][0], min_area_box[1][1]), \\\n","                                    (min_area_box[2][0], min_area_box[2][1]), (0, 30, 255), 3)\n","      draw_min_area_rect = cv2.line(draw_min_area_rect, (min_area_box[2][0], min_area_box[2][1]), \\\n","                                    (min_area_box[3][0], min_area_box[3][1]), (0, 30, 255), 3)\n","      draw_min_area_rect = cv2.line(draw_min_area_rect, (min_area_box[3][0], min_area_box[3][1]), \\\n","                                    (min_area_box[0][0], min_area_box[0][1]), (0, 30, 255), 3)\n","      #Print the angle of rotation in the center of the rectangle\n","      cv2.putText(draw_min_area_rect, str(rect[-1]),\n","                  (int(rect[0][0]) -100, int(rect[0][1])), cv2.FONT_HERSHEY_SIMPLEX,\n","                  1, (0, 30, 255, 255), 3)\n","  else :\n","    min_area_box = cv2.boxPoints(cv2minimumarearectangle)\n","    min_area_box = np.int0(min_area_box)\n","    #This gnarly code again... I should really refactor this into a function,\n","    #because I hate looking at it, and once is bad enough.\n","    draw_min_area_rect = cv2.line(draw_min_area_rect, (min_area_box[0][0], min_area_box[0][1]), \\\n","                                  (min_area_box[1][0], min_area_box[1][1]), (0, 30, 255), 3)\n","    draw_min_area_rect = cv2.line(draw_min_area_rect, (min_area_box[1][0], min_area_box[1][1]), \\\n","                                  (min_area_box[2][0], min_area_box[2][1]), (0, 30, 255), 3)\n","    draw_min_area_rect = cv2.line(draw_min_area_rect, (min_area_box[2][0], min_area_box[2][1]), \\\n","                                  (min_area_box[3][0], min_area_box[3][1]), (0, 30, 255), 3)\n","    draw_min_area_rect = cv2.line(draw_min_area_rect, (min_area_box[3][0], min_area_box[3][1]), \\\n","                                  (min_area_box[0][0], min_area_box[0][1]), (0, 30, 255), 3)\n","    cv2.putText(draw_min_area_rect, str(cv2minimumarearectangle[-1]),\n","                (int(cv2minimumarearectangle[0][0]) -100, int(cv2minimumarearectangle[0][1])),\n","                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 30, 255, 255), 3)\n","\n","  return draw_min_area_rect"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YlqhgCKAdjz3"},"outputs":[],"source":["#Code cell #10\n","#Actually draw the rectangles using the function defined in code cell 9.\n","\n","#Make an image to render\n","draw_rects = show_contours.copy()\n","#An empty list for our minAreaRects\n","rects = []\n","\n","#Iterate through the contours\n","for contour in contours :\n","  #Find the minAreaRect for each contour\n","  minAreaRect = cv2.minAreaRect(contour)\n","  #minAreaRects with a height of less than 60 pixels seemed almost always to be\n","  #artifacts of noise, rather than text blocks we'd actually be interested in.\n","  if minAreaRect[1][1] > 60 :\n","    #Let's ignore any text block that appears to be perfectly horizontal--or\n","    #rotated 90 degrees. On a normal page, any text block in the latter category\n","    #almost always seems to be noise (like a shadow in the gutter or at the\n","    #margin).\n","    if minAreaRect[-1] not in [-0.0, 0.0, -90.0, 90.0] :\n","      #If it's passed both of these tests, add it to our list.\n","      rects.append(minAreaRect)\n","\n","#Draw all of the minAreaRects that met our thresholds\n","for rect in rects :\n","  draw_rects = draw_min_area_rect(rect, draw_rects)\n","cv2_imshow(draw_rects)"]},{"cell_type":"markdown","metadata":{"id":"8xqcEPQPE9Yp"},"source":["###3.d - Calculating an average angle to use for deskewing\n","\n","This cell averages the rotation angles of the `minAreaRect`s in order to figure out how much to rotate the image in order to straighten it.\n","\n",">Note: While updating this notebook, I ran into what I believe may have been some changes introduced in OpenCV which required some modifications to code I wrote a while back. (Alternatively, I may have just uncovered some errors in my earlier code!) I'm not sure I've gotten my head around these changes, entirely, so while I've checked to see that the *output* is correct, I'm not sure my explanations are. For further discussion of this problem, see: https://starecat.com/content/wp-content/uploads/my-code-doesnt-work-i-have-no-idea-why-my-code-works.jpg\n","\n","In addition to knowing what the angles for the blocks of text are, we do need to keep track of which *direction* each block is skewed. This cell uses an I-was-pleasantly-surprised-it-actually-worked method of figuring out the *prevailing* direction of skewing on the page in order to determine whether we should use a positive or negative angle to deskew the page."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wC1Euf7GBlBM"},"outputs":[],"source":["#Code cell #11\n","#Create an empty list that will hold a series of tuples: the first element will\n","#be the angle, and the second will be either 1 or -1)\n","angle_corrections = []\n","for rect in rects :\n","  if 45 < rect[-1] < 90 :\n","    #For angles of more than 45 degrees, construct a tuple with\n","    #the deviation from 90 degrees (rendered as a negative number) as the\n","    #first term (e.g., 90 - (-1 * -87.6834) = 90 - 87.6834 = 2.3166)\n","    #and -1 as the second\n","    angle_corrections.append((-1* (90 - (rect[-1])), -1))\n","  else :\n","    #For angles of less than 45 degrees construct a tuple with\n","    #the deviation from 90 degrees as the first term\n","    #(e.g. 90 - 90 + -1.4596 = 90 - 88.5404 = 1.4596)\n","    #and 1 as the second term\n","    angle_corrections.append((90 - (90 + rect[-1]), 1))\n","#Determine the mean of the first terms of all of the tuples\n","#in our list of angle_corrections. Note this is line is using list comprehension\n","#syntax: https://www.w3schools.com/python/python_lists_comprehension.asp\n","average_angle = np.mean([angle_tuple[0] for angle_tuple in angle_corrections])\n","\n","#Determine whether the deskew angle should be treated as positive\n","#or negative by taking the sum of the second terms of all\n","#of the tuples (e.g., -1 + -1 + -1 + 1 + -1 = -3)\n","plus_or_minus = sum(angle_tuple[1] for angle_tuple in angle_corrections)\n","#If that sum ends up as a positive number, the deskewing angle\n","#needs to be a negative number\n","if plus_or_minus > 0 :\n","  average_angle = -1.0 * average_angle\n","print(plus_or_minus)\n","print(average_angle)"]},{"cell_type":"code","source":["for angle_correction in angle_corrections :\n","  print(angle_correction)"],"metadata":{"id":"RwIzFMAR67-D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y7PMczhaqF9b"},"source":["### 3.e - Let's see the deskewed image.\n","The code in the next cell takes a few steps to rotate our image\n","1. First, we make a copy of our original image\n","2. Next, we determine the size of the image by getting its height and width (the first two items returned by `shape`)\n","3. Then, we determine the center of the image by dividing its height and width by two.\n","4. Next, we construct the rotation we want to happen: rotating the image around its center point by the `angle` we determined in the previous cell.\n","\n","Note how we're using information that we calculated by using what is to us a very strange-looking image, and applying it to our original color image."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R1HoHB64BsSG"},"outputs":[],"source":["#Code cell 12\n","average_angle_deskew = im.copy()\n","(h, w) = average_angle_deskew.shape[:2]\n","center = (w // 2, h // 2)\n","# M = cv2.getRotationMatrix2D(center, angle, 1.0)\n","M = cv2.getRotationMatrix2D(center, average_angle, 1.0)\n","deskewed_average_angle = cv2.warpAffine(average_angle_deskew, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n","cv2_imshow(deskewed_average_angle)"]},{"cell_type":"markdown","metadata":{"id":"bhj9sqWgH2Fk"},"source":["###3.f - Automatically deskewing the image\n","As in the last notebook, we've walked through each step of this process to see how it works. But once we've figured out how to do it, we can write a function to call using a `for` loop."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xNnPSq_FIJ_7"},"outputs":[],"source":["#Code cell 13\n","def deskew_image(image) :\n","  #Convert to gray\n","  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","  #Apply Gaussian blur\n","  blur = cv2.GaussianBlur(gray, (9, 9), 0)\n","  #Invert\n","  thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n","  #Define the kernel\n","  kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (20, 1))\n","  #Dilate\n","  dilate = cv2.dilate(thresh, kernel, iterations=5)\n","  #Determine contours\n","  contours, hierarchy = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","  #Decide which contours to keep\n","  rects = []\n","  for contour in contours :\n","    minAreaRect = cv2.minAreaRect(contour)\n","    if minAreaRect[1][1] > 60 :\n","      if minAreaRect[-1] not in [-0.0, 0.0, -90.0, 90] :\n","        rects.append(minAreaRect)\n","  #Average the angle needed to deskew, and determine in which direction\n","  angle_corrections = []\n","  for rect in rects :\n","    if 45 < rect[-1] < 90 :\n","      angle_corrections.append((-1* (90 - (rect[-1])), -1))\n","    else :\n","      angle_corrections.append((90 - (90 + rect[-1]), 1))\n","  average_angle = np.mean([angle_tuple[0] for angle_tuple in angle_corrections])\n","  plus_or_minus = sum(angle_tuple[1] for angle_tuple in angle_corrections)\n","  if plus_or_minus > 0 :\n","    average_angle = -1.0 * average_angle\n","  #Use averaged angle to rotate the original image around its center\n","  average_angle_deskew = image.copy()\n","  (h, w) = average_angle_deskew.shape[:2]\n","  center = (w // 2, h // 2)\n","  M = cv2.getRotationMatrix2D(center, average_angle, 1.0)\n","  deskewed = cv2.warpAffine(average_angle_deskew, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n","  #Return the deskewed image\n","  return deskewed\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MqFeLXczKTgI"},"outputs":[],"source":["#Code cell 14\n","if not os.path.exists('/content/deskewed/') :\n","  os.makedirs('/content/deskewed/')\n","for file in glob.glob('/content/cropped/*.tif') :\n","  basename = os.path.basename(file)[:-4]\n","  basename += '-deskewed.tif'\n","  original = cv2.imread(file, cv2.IMREAD_COLOR)\n","  deskewed = deskew_image(original)\n","  cv2.imwrite('/content/deskewed/' + basename, deskewed)\n","  print('Saved ' + basename)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e-7ZNs07LuwP"},"outputs":[],"source":["#Code cell 15\n","%cd /content/\n","!zip -r deskewed.zip deskewed/\n","!mv /content/deskewed.zip /gdrive/MyDrive/rbs_digital_approaches_2023/output/deskewed.zip\n"]},{"cell_type":"markdown","metadata":{"id":"Ex7OkTFozYpP"},"source":["##4 - NOW we can automate optimized binarization\n","Now that we can crop and deskew the text block, we can be more confident that the images we produce will be useful for OCR.\n","\n","The next cells return to the code from code cell 6 in the previous notebook, but step through them a little more deliberately to show how one other parameter can affect the images we produce.\n","\n","We'll step through the process of automatic binarization using Otsu's method, then write a function that we can call for each image using a `for` loop."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DChPW_hnM6bJ"},"outputs":[],"source":["#Code cell 16\n","text_block = cv2.imread('/content/deskewed/1730g_p21-cropped-deskewed.tif',\n","                        cv2.IMREAD_COLOR)\n","cv2_imshow(text_block)"]},{"cell_type":"markdown","metadata":{"id":"epHbpniMzdDr"},"source":["###4.a - Convert to grayscale\n","Nothing too exotic here: just throwing away the color information, but keeping everything else."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2pHvIHYVzd3e"},"outputs":[],"source":["#Code cell 17\n","text_block_gray = cv2.cvtColor(text_block, cv2.COLOR_BGR2GRAY)\n","cv2_imshow(text_block_gray)"]},{"cell_type":"markdown","metadata":{"id":"rkD4A9imY29h"},"source":["### 4.b - Applying a Gaussian blur\n","We could simply convert our grayscale image to black and white right now, but there are arguments for applying a slight blur to our image first. While it seems counterintuitive that we would want to make an image blurrier when what we want to do is to recognize text clearly, that blurring can help to minimize the effect of any noise in the image (including, say, tiny flecks in the paper).\n","\n","(Be sure to run the next cell, as it creates a widget for adjusting the blur that we'll apply in the cell following it.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QTBPamUOvPct"},"outputs":[],"source":["#@title Set values for Gaussian blur {display-mode: \"form\"}\n","#@markdown Try adjusting the value that will be used for blurring in the next cell.\n","\n","#@markdown (You only need to run this cell onceâ€”re-running it will simply reset it to the default value. After changing the value of the slider, try re-running the cell below this one.)\n","blur = widgets.IntSlider(min=1, max=31, step=2, value=5, description='Blur')\n","display(blur)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WdlIM0aKzRQV"},"outputs":[],"source":["#Code cell 21\n","text_block_blurred = cv2.GaussianBlur(text_block_gray, (blur.value, blur.value), 0)\n","\n","cv2_imshow(text_block_blurred)"]},{"cell_type":"markdown","metadata":{"id":"fqUncz6da17z"},"source":["### 4.c - Determining an appropriate threshold using Otsu's method\n","We'll experiment with two different methods for automatically thresholding our image. The first, Otsu's method (as best I understand without actually being able to follow the equations) arrives at a threshold level for the image as a whole by noting the distribution of intensities across *all* the pixels of an image and finding the threshold level that optimally divides those intensities into two clusters: the point at which it makes most sense to say \"Everything greater than this belongs together in one group, and everything less than this belongs together in a different group.\" The \"greater than\" group gets turned to black, and the \"less than\" group gets turned to white.\n","\n","This is more or less what we were trying to do experimentally by changing the threshold value in Section 3, but without the trial and error. Once we've cropped the image to eliminate the pronounced black border, Otsu's method seems to work quite well for the kinds of page images we're dealing with, as we'll see in the cell below. (Notice how much better this one looks than the first time we tried it on the uncropped images back in code cell 10,  and notice, too, how different the threshold value turns out to be this time than the 108 that the method produced above. How does it compare to the value you arrived at through trial and error in code cell 8?)\n","\n",">*Note:* Make sure that the blurred image you've created in the cell above looks good to you, since that's what we'll be thresholding here. If you had experimented with the blur until it started looking terrible, now would be the time to set it back to a more sensible level."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P3xzJKfjzUSf"},"outputs":[],"source":["#Code cell 22\n","(T_new, text_block_otsu) = cv2.threshold(text_block_blurred, 0, 255, cv2.THRESH_OTSU)\n","\n","#Output\n","cv2_imshow(text_block_otsu)\n","print('Otsu threshold is: ' + str(T_new))"]},{"cell_type":"markdown","metadata":{"id":"-TU_4l50fWUt"},"source":[">*Note:* When I've timed these things, the conversion using Otsu's method in OpenCV works out to be somewhat slower than using `Pillow` with a manual threshold level. But we can be more confident that this method has found a good threshold for any given image. If we were trying to automate the conversion of scores (or hundreds, or thousands) of images, the tradeoff in speed for adaptability would probably be worth it."]},{"cell_type":"markdown","metadata":{"id":"damKGv18RMYT"},"source":["### 4.d - Binarize the images\n","This ends up being a pretty short function, which we can call with a `for` loop."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q4MZqb931N9b"},"outputs":[],"source":["#Code cell 23\n","def binarize_image(image) :\n","  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","  blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n","  (T, otsu) = cv2.threshold(blurred, 0, 255, cv2.THRESH_OTSU)\n","  return otsu"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v3tCS9xCRdgg"},"outputs":[],"source":["#Code cell 24\n","if not os.path.exists('/content/bw/') :\n","  os.makedirs('/content/bw/')\n","for file in glob.glob('/content/deskewed/*.tif') :\n","  basename = os.path.basename(file)[:-4]\n","  basename += '-bw.tif'\n","  original = cv2.imread(file, cv2.IMREAD_COLOR)\n","  binarized = binarize_image(original)\n","  cv2.imwrite('/content/bw/' + basename, binarized)\n","  print('Saved ' + basename)"]},{"cell_type":"markdown","metadata":{"id":"dA9wM2-GZ5To"},"source":["###4.e - Let's get three more black and white derivatives for experimental purposes\n","We've gone through this whole process so that we could get images that were in a condition to binarize using Otsu's method so that the computer could calculate an optimum thresdhold value for us.\n","\n","In order to see the impact that differing image quality can have on the OCR output, let's create three different derivatives of the same image using different threshold values. We'll try OCRing all three to compare the output."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nduxa6Sfa2mT"},"outputs":[],"source":["#Code cell 25\n","from PIL import Image\n","if not os.path.exists('/content/bw/') :\n","  os.makedirs('/content/bw/')\n","\n","pilcolor_image = Image.open('/content/deskewed/1730f_p21-cropped-deskewed.tif')\n","pilgray_image = pilcolor_image.convert('L')\n","thresh_values = [95, 145, 175]\n","fn = lambda x : 255 if x > thresh else 0\n","for thresh_value in thresh_values :\n","  thresh = thresh_value\n","  pilbinary_image = pilgray_image.convert('L').point(fn, mode='1')\n","  output_filepath = '/content/bw/1730f_p21_manual_'+ str(thresh) + '.tif'\n","  pilbinary_image.save(output_filepath)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nsbV9GCuR4S8"},"outputs":[],"source":["#Code cell 26\n","%cd /content/\n","!zip -r bw.zip bw/\n","!mv /content/bw.zip /gdrive/MyDrive/rbs_digital_approaches_2023/output/bw.zip\n"]},{"cell_type":"markdown","metadata":{"id":"O8Pzv3IDgBFk"},"source":["##5 - Possible limitations of Otsu's method\n","Otsu's method seems to work great for these page image. We could imagine circumstances, though, where the results wouldn't be so good. In an image where the separation between light and dark pixels was less clear, the threshold determined by Otsu's method might yield a result that was difficult to read. This could be the case, for instance, with an image of a page with lots of ink showing through from the other side, or with too much shadow on the page from less-than-ideal photographic circumstances, or, as we'll see in the next cell, with severe foxing of the paper."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2AR2m_cCj9jL"},"outputs":[],"source":["#Code cell #26\n","%cp /gdrive/MyDrive/L-100\\ Digital\\ Approaches\\ to\\ Bibliography\\ \\&\\ Book\\ History-2023/2023_page_images.zip /content/2023_page_images.zip\n","%cd /content/\n","!unzip 2023_page_images.zip\n","foxed = '/content/2023_page_images/st_tz_foxing.jpg'\n","color_foxed = cv2.imread(foxed, cv2.IMREAD_COLOR)\n","cv2_imshow(color_foxed)"]},{"cell_type":"markdown","metadata":{"id":"KFgYo8JZvfZU"},"source":["How does Otsu's method do with this image? Wellll...\n","\n","(Note that this is just an image I found in a [blog post from the New England Document Conservation Center](https://www.nedcc.org/about/nedcc-stories/story-tz-interview), and not an image that was created through careful digitization. Still, if you've spent any time working with scanned books, you've surely seen something like this before.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6nXWXaKqvlWv"},"outputs":[],"source":["#Code cell #27\n","gray_foxed = cv2.cvtColor(color_foxed, cv2.COLOR_BGR2GRAY)\n","blurred_foxed = cv2.GaussianBlur(gray_foxed, (5, 5), 0)\n","otsu_foxed = cv2.threshold(blurred_foxed, 0, 255, cv2.THRESH_OTSU)[1]\n","\n","cv2_imshow(otsu_foxed)"]},{"cell_type":"markdown","metadata":{"id":"Wd_k0IZQm83k"},"source":["### 6.e - Applying adaptive thresholding for problematic images\n","Rather than attempting to calculate a single threshold point appropriate for the image as a whole, adaptive thresholding first divides the image into segments based on the levels in different regions of the image and then calculates a separate threshold point for each segment. For a generally good image like our pages of *Sophonisba*, the difference isn't really all that noticeable. We can certainly see a difference, but it's not terribly dramatic."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1DBFujh6oXre"},"outputs":[],"source":["#Code cell #28\n","cv2binary_adaptive_image = cv2.adaptiveThreshold(text_block_gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 101, 30)\n","cv2_imshow(cv2binary_adaptive_image)"]},{"cell_type":"markdown","metadata":{"id":"Qr_oox3JoeoH"},"source":["But it makes a *remakable* difference for the badly foxed title page we saw giving Otsu's method trouble:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WTQuRUDTwUEZ"},"outputs":[],"source":["#Code cell #29\n","cv2binary_adaptive_image = cv2.adaptiveThreshold(blurred_foxed, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 101, 30)\n","cv2_imshow(cv2binary_adaptive_image)"]},{"cell_type":"markdown","metadata":{"id":"xO8U-v1pzTQ5"},"source":["## Takeaways\n","Images get converted to black and white for various reasons. At a practical level, the file sizes for black and white images are much smaller than for color images. So if the aim is simply to have a readable text, black and white images get the job done with less storage.\n","\n","But black and white images are also easier for computers to work with in a number of ways, so images end up getting converted a lot for digital work.\n","\n","Different images respond better to different treatments. Automated methods for figuring out likely-optimal values for thresholding any given image are necessary to do this kind of work at scale, but can lead to sometimes unexpected results.\n","\n","When you're working with digital surrogates that have been converted to black and white, it's important to think about how the images may have been transformed so that you can try to work backwards to an at least plausible mental picture of the properties of the source item."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}